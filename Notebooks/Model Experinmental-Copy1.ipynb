{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a77324a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the dataset\n",
    "df = pd.read_csv('1900_2021_DISASTERS_INTERPOLATED.csv')\n",
    "df2 =df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "24778167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Unnamed: 0 Disaster Subgroup Disaster Type  Disaster Subtype  \\\n",
       "0               0    climatological       drought           drought   \n",
       "1               1    climatological       drought           drought   \n",
       "2               2       geophysical    earthquake   ground movement   \n",
       "3               7    meteorological         storm  tropical cyclone   \n",
       "4               9       geophysical    earthquake   ground movement   \n",
       "...           ...               ...           ...               ...   \n",
       "14931       16121      hydrological         flood               NaN   \n",
       "14932       16122      hydrological         flood               NaN   \n",
       "14933       16123        biological      epidemic     viral disease   \n",
       "14934       16124      hydrological         flood               NaN   \n",
       "14935       16125      hydrological         flood               NaN   \n",
       "\n",
       "      Disaster Subsubtype  Event Name                                 Country  \\\n",
       "0                     NaN         NaN                              cabo verde   \n",
       "1                     NaN         NaN                                   india   \n",
       "2                     NaN         NaN                               guatemala   \n",
       "3                     NaN         NaN                              bangladesh   \n",
       "4                     NaN         NaN                                   india   \n",
       "...                   ...         ...                                     ...   \n",
       "14931                 NaN         NaN                                   yemen   \n",
       "14932                 NaN         NaN                            south africa   \n",
       "14933                 NaN  meningitis  congo (the democratic republic of the)   \n",
       "14934                 NaN         NaN                                  serbia   \n",
       "14935                 NaN         NaN                             south sudan   \n",
       "\n",
       "       ISO           Region Continent  ... scaled_Dis Mag Value  \\\n",
       "0      cpv   western africa    africa  ...             0.000004   \n",
       "1      ind    southern asia      asia  ...             0.000004   \n",
       "2      gtm  central america  americas  ...             0.000005   \n",
       "3      bgd    southern asia      asia  ...             0.000020   \n",
       "4      ind    southern asia      asia  ...             0.000005   \n",
       "...    ...              ...       ...  ...                  ...   \n",
       "14931  yem     western asia      asia  ...             0.000004   \n",
       "14932  zaf  southern africa    africa  ...             0.000004   \n",
       "14933  cod    middle africa    africa  ...             0.000227   \n",
       "14934  srb  southern europe    europe  ...             0.000004   \n",
       "14935  ssd  northern africa    africa  ...             0.000004   \n",
       "\n",
       "      scaled_Latitude scaled_Longitude scaled_Total Deaths scaled_No Injured  \\\n",
       "0            0.539987         0.428283            0.642785          0.642785   \n",
       "1            0.599630         0.717217            0.648276          0.648276   \n",
       "2            0.536095         0.241608            0.732441          0.732441   \n",
       "3            0.619594         0.749932            0.000000          0.000000   \n",
       "4            0.599630         0.717217            0.739873          0.739873   \n",
       "...               ...              ...                 ...               ...   \n",
       "14931        0.543247         0.630612            0.540810          0.540810   \n",
       "14932        0.119111         0.566171            0.663810          0.663810   \n",
       "14933        0.361731         0.562880            0.877871          0.877871   \n",
       "14934        0.804377         0.553676            0.062337          0.062337   \n",
       "14935        0.463637         0.579327            0.516639          0.516639   \n",
       "\n",
       "      scaled_No Affected scaled_No Homeless  scaled_Total Affected  \\\n",
       "0               0.642785           0.642785               0.642785   \n",
       "1               0.648276           0.648276               0.648276   \n",
       "2               0.732441           0.732441               0.732441   \n",
       "3               0.000000           0.000000               0.000000   \n",
       "4               0.739873           0.739873               0.739873   \n",
       "...                  ...                ...                    ...   \n",
       "14931           0.540810           0.540810               0.540810   \n",
       "14932           0.663810           0.663810               0.663810   \n",
       "14933           0.877871           0.877871               0.877871   \n",
       "14934           0.062337           0.062337               0.062337   \n",
       "14935           0.516639           0.516639               0.516639   \n",
       "\n",
       "       scaled_CPI scaled_Rank  \n",
       "0        0.000000    0.642785  \n",
       "1        0.000000    0.648276  \n",
       "2        0.001332    0.732441  \n",
       "3        0.002663    0.000000  \n",
       "4        0.002663    0.739873  \n",
       "...           ...         ...  \n",
       "14931    1.000000    0.540810  \n",
       "14932    1.000000    0.663810  \n",
       "14933    1.000000    0.877871  \n",
       "14934    1.000000    0.062337  \n",
       "14935    1.000000    0.516639  \n",
       "\n",
       "[14936 rows x 45 columns]>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Values taken as input are scaled using one hot encoding\n",
    "## Numerical Values Taken as input are converted using min max scaler \n",
    "\n",
    "# Assuming your data is stored in a Pandas dataframe called \"df\"\n",
    "\n",
    "# Select the features that you want to scale\n",
    "features = df[['Dis Mag Value', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'CPI', 'Rank']]\n",
    "\n",
    "# Iterate through each feature\n",
    "for feature in features:\n",
    "  # Select the feature\n",
    "  col = df[feature]\n",
    "\n",
    "  # Find the minimum and maximum values for the column\n",
    "  min_value = col.min()\n",
    "  max_value = col.max()\n",
    "\n",
    "  # Subtract the minimum value from the column\n",
    "  scaled_col = col - min_value\n",
    "\n",
    "  # Divide by the range (max - min)\n",
    "  scaled_col = scaled_col / (max_value - min_value)\n",
    "\n",
    "  # Assign the scaled values to a new column in the dataframe\n",
    "  df[f'scaled_{feature}'] = scaled_col\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df6e703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Disaster Subtype', 'Disaster Subsubtype', 'Event Name',\n",
       "       'Location', 'Origin', 'Associated Dis', 'Associated Dis2',\n",
       "       'OFDA Response', 'Appeal',\n",
       "       ...\n",
       "       'Timezone_Pacific/Niue', 'Timezone_Pacific/Noumea',\n",
       "       'Timezone_Pacific/Pago_Pago', 'Timezone_Pacific/Palau',\n",
       "       'Timezone_Pacific/Port_Moresby', 'Timezone_Pacific/Saipan',\n",
       "       'Timezone_Pacific/Tahiti', 'Timezone_Pacific/Tarawa',\n",
       "       'Timezone_Pacific/Tongatapu', 'Timezone_Pacific/Wallis'],\n",
       "      dtype='object', length=738)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, columns=['Disaster Subgroup','Country', 'Disaster Type', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eeec6c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Aid Contribution', 'Dis Mag Value', 'Latitude',\n",
       "       'Longitude', 'Total Deaths', 'No Injured', 'No Affected', 'No Homeless',\n",
       "       'Total Affected',\n",
       "       ...\n",
       "       'Timezone_Pacific/Niue', 'Timezone_Pacific/Noumea',\n",
       "       'Timezone_Pacific/Pago_Pago', 'Timezone_Pacific/Palau',\n",
       "       'Timezone_Pacific/Port_Moresby', 'Timezone_Pacific/Saipan',\n",
       "       'Timezone_Pacific/Tahiti', 'Timezone_Pacific/Tarawa',\n",
       "       'Timezone_Pacific/Tongatapu', 'Timezone_Pacific/Wallis'],\n",
       "      dtype='object', length=727)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e24cb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "374/374 [==============================] - 1s 893us/step - loss: 26756526.0000 - mae: 3055.2742\n",
      "Epoch 2/50\n",
      "374/374 [==============================] - 0s 909us/step - loss: 2991205.2500 - mae: 811.9517\n",
      "Epoch 3/50\n",
      "374/374 [==============================] - 0s 898us/step - loss: 1342448.8750 - mae: 505.2916\n",
      "Epoch 4/50\n",
      "374/374 [==============================] - 0s 939us/step - loss: 1014510.1875 - mae: 449.3370\n",
      "Epoch 5/50\n",
      "374/374 [==============================] - 0s 911us/step - loss: 812937.0625 - mae: 398.5540\n",
      "Epoch 6/50\n",
      "374/374 [==============================] - 0s 931us/step - loss: 659911.0000 - mae: 346.1614\n",
      "Epoch 7/50\n",
      "374/374 [==============================] - 0s 949us/step - loss: 546261.0000 - mae: 303.8280\n",
      "Epoch 8/50\n",
      "374/374 [==============================] - 0s 1ms/step - loss: 461624.2500 - mae: 269.7254\n",
      "Epoch 9/50\n",
      "374/374 [==============================] - 0s 1ms/step - loss: 395346.5000 - mae: 241.1055\n",
      "Epoch 10/50\n",
      "374/374 [==============================] - 0s 889us/step - loss: 342075.6875 - mae: 216.8638\n",
      "Epoch 11/50\n",
      "374/374 [==============================] - 0s 852us/step - loss: 297577.9688 - mae: 192.3055\n",
      "Epoch 12/50\n",
      "374/374 [==============================] - 0s 874us/step - loss: 259138.4688 - mae: 165.6720\n",
      "Epoch 13/50\n",
      "374/374 [==============================] - 0s 999us/step - loss: 227167.1562 - mae: 141.6269\n",
      "Epoch 14/50\n",
      "374/374 [==============================] - 0s 947us/step - loss: 204717.6875 - mae: 124.5937\n",
      "Epoch 15/50\n",
      "374/374 [==============================] - 0s 874us/step - loss: 189093.0625 - mae: 114.4751\n",
      "Epoch 16/50\n",
      "374/374 [==============================] - 0s 904us/step - loss: 175430.6250 - mae: 106.2120\n",
      "Epoch 17/50\n",
      "374/374 [==============================] - 0s 949us/step - loss: 162119.4844 - mae: 98.6841\n",
      "Epoch 18/50\n",
      "374/374 [==============================] - 0s 834us/step - loss: 149919.2656 - mae: 93.5622\n",
      "Epoch 19/50\n",
      "374/374 [==============================] - 0s 826us/step - loss: 138696.0156 - mae: 88.0209\n",
      "Epoch 20/50\n",
      "374/374 [==============================] - 0s 1ms/step - loss: 128135.8828 - mae: 83.3506\n",
      "Epoch 21/50\n",
      "374/374 [==============================] - 0s 855us/step - loss: 118765.5938 - mae: 79.1453\n",
      "Epoch 22/50\n",
      "374/374 [==============================] - 0s 813us/step - loss: 109944.9297 - mae: 76.3597\n",
      "Epoch 23/50\n",
      "374/374 [==============================] - 0s 823us/step - loss: 101673.1250 - mae: 72.2129\n",
      "Epoch 24/50\n",
      "374/374 [==============================] - 0s 1ms/step - loss: 94734.4062 - mae: 69.1113\n",
      "Epoch 25/50\n",
      "374/374 [==============================] - 0s 802us/step - loss: 88554.5391 - mae: 65.8942\n",
      "Epoch 26/50\n",
      "374/374 [==============================] - 0s 893us/step - loss: 83714.9219 - mae: 62.5571\n",
      "Epoch 27/50\n",
      "374/374 [==============================] - 0s 858us/step - loss: 79927.4062 - mae: 60.5782\n",
      "Epoch 28/50\n",
      "374/374 [==============================] - 0s 901us/step - loss: 76752.9766 - mae: 57.3561\n",
      "Epoch 29/50\n",
      "374/374 [==============================] - 0s 853us/step - loss: 73712.2969 - mae: 55.6493\n",
      "Epoch 30/50\n",
      "374/374 [==============================] - 0s 842us/step - loss: 71120.1641 - mae: 53.0756\n",
      "Epoch 31/50\n",
      "374/374 [==============================] - 0s 896us/step - loss: 68678.2891 - mae: 50.9569\n",
      "Epoch 32/50\n",
      "374/374 [==============================] - 0s 822us/step - loss: 67054.8125 - mae: 50.3634\n",
      "Epoch 33/50\n",
      "374/374 [==============================] - 0s 802us/step - loss: 65078.0977 - mae: 48.5123\n",
      "Epoch 34/50\n",
      "374/374 [==============================] - 0s 845us/step - loss: 63554.2383 - mae: 47.5734\n",
      "Epoch 35/50\n",
      "374/374 [==============================] - 0s 880us/step - loss: 62213.1836 - mae: 47.1055\n",
      "Epoch 36/50\n",
      "374/374 [==============================] - 0s 936us/step - loss: 60570.4648 - mae: 45.5472\n",
      "Epoch 37/50\n",
      "374/374 [==============================] - 0s 944us/step - loss: 59398.3594 - mae: 44.7168\n",
      "Epoch 38/50\n",
      "374/374 [==============================] - 0s 1ms/step - loss: 58240.7188 - mae: 44.7497\n",
      "Epoch 39/50\n",
      "374/374 [==============================] - 0s 863us/step - loss: 56758.2930 - mae: 43.1417\n",
      "Epoch 40/50\n",
      "374/374 [==============================] - 0s 955us/step - loss: 55686.9922 - mae: 42.7614\n",
      "Epoch 41/50\n",
      "374/374 [==============================] - 0s 898us/step - loss: 54786.4922 - mae: 42.3564\n",
      "Epoch 42/50\n",
      "374/374 [==============================] - 0s 834us/step - loss: 53915.8125 - mae: 42.1677\n",
      "Epoch 43/50\n",
      "374/374 [==============================] - 0s 804us/step - loss: 53162.6914 - mae: 41.1406\n",
      "Epoch 44/50\n",
      "374/374 [==============================] - 0s 847us/step - loss: 52379.0625 - mae: 41.1626\n",
      "Epoch 45/50\n",
      "374/374 [==============================] - 0s 898us/step - loss: 51881.5078 - mae: 40.9515\n",
      "Epoch 46/50\n",
      "374/374 [==============================] - 0s 979us/step - loss: 51417.1289 - mae: 40.8353\n",
      "Epoch 47/50\n",
      "374/374 [==============================] - 0s 989us/step - loss: 50772.8203 - mae: 40.2899\n",
      "Epoch 48/50\n",
      "374/374 [==============================] - 0s 877us/step - loss: 50193.3516 - mae: 39.2680\n",
      "Epoch 49/50\n",
      "374/374 [==============================] - 0s 842us/step - loss: 49790.5039 - mae: 39.0916\n",
      "Epoch 50/50\n",
      "374/374 [==============================] - 0s 933us/step - loss: 49231.6250 - mae: 38.2080\n",
      "94/94 [==============================] - 0s 710us/step - loss: 87932.1719 - mae: 50.0629\n",
      "loss\n",
      "mae\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# select the features and target columns\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, return_dict=True)\n",
    "#print(f'Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "#print('Test loss: {:.4f}, Test MAE: {:.4f}'.format(test_loss, test_mae))\n",
    "print(test_loss)\n",
    "print(test_mae)\n",
    "\n",
    "\n",
    "# use the model to make predictions on new data\n",
    "#new_data = np.array([[...]]) # replace with actual data\n",
    "#predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "045acc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# extract the MAE values from the history\n",
    "mae = history.history['mae']\n",
    "\n",
    "# plot the MAE values\n",
    "#plt.plot(mae)\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('MAE')\n",
    "#plt.show()\n",
    "\n",
    "# create a scatter plot with the epoch numbers on the x-axis and the MAE values on the y-axis\n",
    "plt.scatter(range(1, len(mae) + 1), mae)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bf8a6749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.04484605087014726\n"
     ]
    }
   ],
   "source": [
    "##Not working Rn  \n",
    "#Need to scale the target values for this to work \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data and split it into training and testing sets\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df['Rank']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c8a6576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.79272462393702e-18\n"
     ]
    }
   ],
   "source": [
    "##Should work with scaled values \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "#X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Total Deaths']\n",
    "\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[ 'scaled_Dis Mag Value']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9ef6c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "299/299 [==============================] - 1s 2ms/step - loss: 37177476.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177504.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177492.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177476.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177488.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177492.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177484.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177476.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177472.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177484.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177476.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177476.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 19/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 20/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177488.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 21/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177492.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 22/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 23/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 24/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177472.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 25/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 26/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 27/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 28/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 29/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177488.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 30/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177460.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 31/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37177484.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 32/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 33/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177472.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 34/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177472.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 35/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37177484.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 36/40\n",
      "299/299 [==============================] - 1s 2ms/step - loss: 37177468.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 37/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177476.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 38/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177500.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 39/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177492.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "Epoch 40/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37177480.0000 - accuracy: 0.0254 - val_loss: 37497908.0000 - val_accuracy: 0.0285\n",
      "94/94 [==============================] - 0s 764us/step - loss: 36845992.0000 - accuracy: 0.0278\n",
      "Test loss: 36845992.0000\n",
      "Test accuracy: 0.03\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_logarithmic_error'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8695cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umerf\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1696: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299/299 [==============================] - 1s 2ms/step - loss: -12753143028776960.0000 - accuracy: 0.0000e+00 - val_loss: -73166752162250752.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -553553664489291776.0000 - accuracy: 0.0000e+00 - val_loss: -1506407620989157376.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -3978883541537652736.0000 - accuracy: 0.0000e+00 - val_loss: -7697781511373717504.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -14300722120366227456.0000 - accuracy: 0.0000e+00 - val_loss: -23236836848859873280.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -36424902679940038656.0000 - accuracy: 0.0000e+00 - val_loss: -53283485770210869248.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -75446324059105656832.0000 - accuracy: 0.0000e+00 - val_loss: -102871812026629357568.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -136291318419299500032.0000 - accuracy: 0.0000e+00 - val_loss: -176952639401060466688.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -223990468116945567744.0000 - accuracy: 0.0000e+00 - val_loss: -280364276452555751424.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "299/299 [==============================] - 0s 2ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "94/94 [==============================] - 0s 796us/step - loss: nan - accuracy: 0.0000e+00\n",
      "Test loss: nan\n",
      "Test accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "\n",
    "#X = df.drop([['Aid Contribution', 'Dis Mag Value', 'Latitude',\n",
    " #      'Longitude', 'Total Deaths', 'No Injured', 'No Affected', 'No Homeless',\n",
    "  #'Total Affected']])\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "\n",
    "y = df['Total Deaths']\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b7d81d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Disaster Subgroup', 'Disaster Type', 'Disaster Subtype',\n",
       "       'Disaster Subsubtype', 'Event Name', 'Country', 'ISO', 'Region',\n",
       "       'Continent', 'Location', 'Origin', 'Associated Dis', 'Associated Dis2',\n",
       "       'OFDA Response', 'Appeal', 'Declaration', 'Aid Contribution',\n",
       "       'Dis Mag Value', 'Dis Mag Scale', 'Latitude', 'Longitude', 'Timezone',\n",
       "       'River Basin', 'Total Deaths', 'No Injured', 'No Affected',\n",
       "       'No Homeless', 'Total Affected', 'Insured Damages ('000 US$)',\n",
       "       'Total Damages ('000 US$)', 'CPI', 'Start Date', 'End Date', 'Rank',\n",
       "       'scaled_Dis Mag Value', 'scaled_Latitude', 'scaled_Longitude',\n",
       "       'scaled_Total Deaths', 'scaled_No Injured', 'scaled_No Affected',\n",
       "       'scaled_No Homeless', 'scaled_Total Affected', 'scaled_CPI',\n",
       "       'scaled_Rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d2e00f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Disaster Subtype', 'Disaster Subsubtype', 'Event Name',\n",
       "       'Country', 'Location', 'Origin', 'Associated Dis', 'Associated Dis2',\n",
       "       'OFDA Response',\n",
       "       ...\n",
       "       'Timezone_Pacific/Niue', 'Timezone_Pacific/Noumea',\n",
       "       'Timezone_Pacific/Pago_Pago', 'Timezone_Pacific/Palau',\n",
       "       'Timezone_Pacific/Port_Moresby', 'Timezone_Pacific/Saipan',\n",
       "       'Timezone_Pacific/Tahiti', 'Timezone_Pacific/Tarawa',\n",
       "       'Timezone_Pacific/Tongatapu', 'Timezone_Pacific/Wallis'],\n",
       "      dtype='object', length=511)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.get_dummies(df2, columns=['Disaster Subgroup', 'Disaster Type', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00f03bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0635402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umerf\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "## Experimental This is not working \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume you have a dataframe 'df' with columns 'feature1', 'feature2', ..., 'target'\n",
    "# and a list of the column names of the features called 'feature_cols'\n",
    "\n",
    "# Split the data into features and target\n",
    "#X = df[['ISO', 'Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "\n",
    "X = df2.drop(['Country','Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "\n",
    "y = df2['Country']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the 'predict' method to make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# You can also use the 'predict_proba' method to predict the class probabilities\n",
    "probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data using the 'accuracy_score' function from the 'sklearn.metrics' module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encoding using one hot encoding\n",
    "df = pd.get_dummies(df, columns=['Disaster Subgroup', 'Disaster Type', 'Country', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bdaa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling using Min Max \n",
    "# Assuming your data is stored in a Pandas dataframe called \"df\"\n",
    "\n",
    "# Select the features that you want to scale\n",
    "features = df[['Dis Mag Value', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'CPI', 'Rank']]\n",
    "\n",
    "# Iterate through each feature\n",
    "for feature in features:\n",
    "  # Select the feature\n",
    "  col = df[feature]\n",
    "\n",
    "  # Find the minimum and maximum values for the column\n",
    "  min_value = col.min()\n",
    "  max_value = col.max()\n",
    "\n",
    "  # Subtract the minimum value from the column\n",
    "  scaled_col = col - min_value\n",
    "\n",
    "  # Divide by the range (max - min)\n",
    "  scaled_col = scaled_col / (max_value - min_value)\n",
    "\n",
    "  # Assign the scaled values to a new column in the dataframe\n",
    "  df[f'scaled_{feature}'] = scaled_col\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# select the features and target columns\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, return_dict=True)\n",
    "#print(f'Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "#print('Test loss: {:.4f}, Test MAE: {:.4f}'.format(test_loss, test_mae))\n",
    "print(test_loss)\n",
    "print(test_mae)\n",
    "\n",
    "\n",
    "# use the model to make predictions on new data\n",
    "#new_data = np.array([[...]]) # replace with actual data\n",
    "#predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Need to preprocess the data first\n",
    "\n",
    "# create a Pandas data frame with the input data for the new samples\n",
    "lol = df = pd.read_csv('2022_DISASTERS_TARGET.csv')\n",
    "\n",
    "\n",
    "# convert the data frame to a NumPy array\n",
    "new_data = lol.values\n",
    "\n",
    "# use the model to make predictions on the new data\n",
    "predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb494191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# extract the MAE values from the history\n",
    "mae = history.history['mae']\n",
    "\n",
    "# plot the MAE values\n",
    "plt.plot(mae)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()\n",
    "\n",
    "# create a scatter plot with the epoch numbers on the x-axis and the MAE values on the y-axis\n",
    "#plt.scatter(range(1, len(mae) + 1), mae)\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('MAE')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "#X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Total Deaths']\n",
    "\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[ 'scaled_Dis Mag Value']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_logarithmic_error'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1356b6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Select the categorical columns\n",
    "cat_cols = df[['Disaster Subgroup', 'Disaster Type', 'Country', 'ISO', 'Region', 'Continent','Dis Mag Scale']]\n",
    "\n",
    "# Create the OneHotEncoder object\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "onehot_encoded = onehot_encoder.fit_transform(df[cat_cols])\n",
    "\n",
    "# Add the one-hot encoded columns to the original dataframe\n",
    "df_onehot = pd.concat([df, pd.DataFrame(onehot_encoded)], axis=1)\n",
    "\n",
    "# Drop the categorical columns\n",
    "df_onehot = df_onehot.drop(cat_cols, axis=1)\n",
    "\n",
    "\n",
    "# Display the resulting dataframe\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577f0572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Select the numerical columns\n",
    "num_cols = ['Aid Contribution', 'Dis Mag Value', 'Dis Mag Scale', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "            'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'Insured Damages ('000 US$)', \n",
    "            'Total Damages ('000 US$)', 'CPI']\n",
    "\n",
    "# Create the MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the numerical columns\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db19c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() \n",
    "df['Disaster Type']= le.fit_transform(df['Disaster Type'])\n",
    "df['Disaster Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3428db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing categorical values to numerical values\n",
    "dfle = df\n",
    "dfle.Continent = le.fit_transform(dfle.Continent)\n",
    "dfle.ISO= le.fit_transform(dfle.ISO)\n",
    "dfle.Region= le.fit_transform(dfle.Region)\n",
    "dfle['Disaster Type']= le.fit_transform(dfle['Disaster Type'])\n",
    "dfle['Disaster Subgroup']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "dfle['Start Date']= le.fit_transform(dfle['Start Date'])\n",
    "dfle['End Date']= le.fit_transform(dfle['End Date'])\n",
    "dfle['Timezone']= le.fit_transform(dfle['Timezone'])\n",
    "dfle['Country']= le.fit_transform(dfle['Country'])\n",
    "dfle['Dis Mag Scale']= le.fit_transform(dfle['Dis Mag Scale'])\n",
    "dfle.head()\n",
    "dfle.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the 'Date' column to a datetime type\n",
    "# df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# # Extract the month from each date\n",
    "# df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "# F = df3[['Year','Longitude','Latitude']]\n",
    "# #F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae42349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the 'Date' column to a datetime type\n",
    "# df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# # Extract the month from each date\n",
    "# df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "# F = df3[['Year','Longitude','Latitude']]\n",
    "# #F\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the dataset and extract the input features and target variable\n",
    "# #X = dfle[['ISO','Total Deaths','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "# #y = dfle['Country']\n",
    "\n",
    "# X = df3.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "# #y = df3['Rank']\n",
    "# y = F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Split the data into a training set and a test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# # Train a random forest classifier on the training set\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model's performance using accuracy\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "#X = df3.drop(['Total Deaths'], axis=1)  \n",
    "#y = df3['Total Deaths']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41761704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume you have a dataframe 'df' with columns 'feature1', 'feature2', ..., 'target'\n",
    "# and a list of the column names of the features called 'feature_cols'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = dfle[['ISO', 'Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "y = dfle['Timezone']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the 'predict' method to make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# You can also use the 'predict_proba' method to predict the class probabilities\n",
    "probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data using the 'accuracy_score' function from the 'sklearn.metrics' module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
