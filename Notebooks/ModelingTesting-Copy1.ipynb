{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fd76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20cf0eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Disaster Subgroup</th>\n",
       "      <th>Disaster Type</th>\n",
       "      <th>Disaster Subtype</th>\n",
       "      <th>Disaster Subsubtype</th>\n",
       "      <th>Event Name</th>\n",
       "      <th>Country</th>\n",
       "      <th>ISO</th>\n",
       "      <th>Region</th>\n",
       "      <th>Continent</th>\n",
       "      <th>...</th>\n",
       "      <th>No Injured</th>\n",
       "      <th>No Affected</th>\n",
       "      <th>No Homeless</th>\n",
       "      <th>Total Affected</th>\n",
       "      <th>Insured Damages ('000 US$)</th>\n",
       "      <th>Total Damages ('000 US$)</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>climatological</td>\n",
       "      <td>drought</td>\n",
       "      <td>drought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cabo verde</td>\n",
       "      <td>cpv</td>\n",
       "      <td>western africa</td>\n",
       "      <td>africa</td>\n",
       "      <td>...</td>\n",
       "      <td>1.157057e+06</td>\n",
       "      <td>2.121271e+08</td>\n",
       "      <td>1.018853e+07</td>\n",
       "      <td>2.121271e+08</td>\n",
       "      <td>5.111317e+07</td>\n",
       "      <td>1.553171e+08</td>\n",
       "      <td>3.221647</td>\n",
       "      <td>1900-07-15</td>\n",
       "      <td>1900-07-16</td>\n",
       "      <td>9601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>climatological</td>\n",
       "      <td>drought</td>\n",
       "      <td>drought</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>ind</td>\n",
       "      <td>southern asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>...</td>\n",
       "      <td>1.166939e+06</td>\n",
       "      <td>2.139388e+08</td>\n",
       "      <td>1.027555e+07</td>\n",
       "      <td>2.139388e+08</td>\n",
       "      <td>5.154972e+07</td>\n",
       "      <td>1.566437e+08</td>\n",
       "      <td>3.221647</td>\n",
       "      <td>1900-07-15</td>\n",
       "      <td>1900-07-16</td>\n",
       "      <td>9683.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>geophysical</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>ground movement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guatemala</td>\n",
       "      <td>gtm</td>\n",
       "      <td>central america</td>\n",
       "      <td>americas</td>\n",
       "      <td>...</td>\n",
       "      <td>1.318426e+06</td>\n",
       "      <td>2.417113e+08</td>\n",
       "      <td>1.160947e+07</td>\n",
       "      <td>2.417113e+08</td>\n",
       "      <td>5.824165e+07</td>\n",
       "      <td>1.769784e+08</td>\n",
       "      <td>3.350513</td>\n",
       "      <td>1902-04-18</td>\n",
       "      <td>1902-04-18</td>\n",
       "      <td>10940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>meteorological</td>\n",
       "      <td>storm</td>\n",
       "      <td>tropical cyclone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bangladesh</td>\n",
       "      <td>bgd</td>\n",
       "      <td>southern asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>...</td>\n",
       "      <td>1.215141e+02</td>\n",
       "      <td>2.209527e+04</td>\n",
       "      <td>1.067194e+03</td>\n",
       "      <td>2.209527e+04</td>\n",
       "      <td>5.364080e+03</td>\n",
       "      <td>1.617936e+04</td>\n",
       "      <td>3.479379</td>\n",
       "      <td>1904-11-18</td>\n",
       "      <td>1904-11-18</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>geophysical</td>\n",
       "      <td>earthquake</td>\n",
       "      <td>ground movement</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>india</td>\n",
       "      <td>ind</td>\n",
       "      <td>southern asia</td>\n",
       "      <td>asia</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331803e+06</td>\n",
       "      <td>2.441638e+08</td>\n",
       "      <td>1.172726e+07</td>\n",
       "      <td>2.441638e+08</td>\n",
       "      <td>5.883258e+07</td>\n",
       "      <td>1.787741e+08</td>\n",
       "      <td>3.479379</td>\n",
       "      <td>1905-04-04</td>\n",
       "      <td>1905-04-04</td>\n",
       "      <td>11051.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Disaster Subgroup Disaster Type  Disaster Subtype  \\\n",
       "0           0    climatological       drought           drought   \n",
       "1           1    climatological       drought           drought   \n",
       "2           2       geophysical    earthquake   ground movement   \n",
       "3           7    meteorological         storm  tropical cyclone   \n",
       "4           9       geophysical    earthquake   ground movement   \n",
       "\n",
       "  Disaster Subsubtype Event Name     Country  ISO           Region Continent  \\\n",
       "0                 NaN        NaN  cabo verde  cpv   western africa    africa   \n",
       "1                 NaN        NaN       india  ind    southern asia      asia   \n",
       "2                 NaN        NaN   guatemala  gtm  central america  americas   \n",
       "3                 NaN        NaN  bangladesh  bgd    southern asia      asia   \n",
       "4                 NaN        NaN       india  ind    southern asia      asia   \n",
       "\n",
       "   ...    No Injured   No Affected   No Homeless Total Affected  \\\n",
       "0  ...  1.157057e+06  2.121271e+08  1.018853e+07   2.121271e+08   \n",
       "1  ...  1.166939e+06  2.139388e+08  1.027555e+07   2.139388e+08   \n",
       "2  ...  1.318426e+06  2.417113e+08  1.160947e+07   2.417113e+08   \n",
       "3  ...  1.215141e+02  2.209527e+04  1.067194e+03   2.209527e+04   \n",
       "4  ...  1.331803e+06  2.441638e+08  1.172726e+07   2.441638e+08   \n",
       "\n",
       "  Insured Damages ('000 US$) Total Damages ('000 US$)       CPI  Start Date  \\\n",
       "0               5.111317e+07             1.553171e+08  3.221647  1900-07-15   \n",
       "1               5.154972e+07             1.566437e+08  3.221647  1900-07-15   \n",
       "2               5.824165e+07             1.769784e+08  3.350513  1902-04-18   \n",
       "3               5.364080e+03             1.617936e+04  3.479379  1904-11-18   \n",
       "4               5.883258e+07             1.787741e+08  3.479379  1905-04-04   \n",
       "\n",
       "     End Date     Rank  \n",
       "0  1900-07-16   9601.0  \n",
       "1  1900-07-16   9683.0  \n",
       "2  1902-04-18  10940.0  \n",
       "3  1904-11-18      1.0  \n",
       "4  1905-04-04  11051.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('1900_2021_DISASTERS_INTERPOLATED.csv')\n",
    "\n",
    "# View the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c2bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db3e8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb05ca55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Disaster Subgroup', 'Disaster Type', 'Country', 'ISO',\n",
       "       'Region', 'Continent', 'Aid Contribution', 'Dis Mag Value',\n",
       "       'Dis Mag Scale', 'Latitude', 'Longitude', 'Timezone', 'Total Deaths',\n",
       "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected',\n",
       "       'Insured Damages ('000 US$)', 'Total Damages ('000 US$)', 'CPI',\n",
       "       'Start Date', 'End Date', 'Rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=1)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "608c8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df \n",
    "# One hot encode the categorical variables\n",
    "#df_subgroup = pd.get_dummies(df['Disaster Subgroup'], prefix='Disaster Subgroup')\n",
    "#df_type = pd.get_dummies(df['Disaster Type'], prefix='Disaster Type')\n",
    "#df_country = pd.get_dummies(df['Country'], prefix='Country')\n",
    "#df_ISO = pd.get_dummies(df['ISO'], prefix='ISO')\n",
    "#df_region = pd.get_dummies(df['Region'], prefix='Region')\n",
    "#df_continent = pd.get_dummies(df['Continent'], prefix='Continent')\n",
    "\n",
    "# Concatenate the one hot encoded dataframes\n",
    "#df_one_hot_encoded = pd.concat([df_subgroup, df_type, df_country, df_ISO, df_region, df_continent], axis=1)\n",
    "\n",
    "# Convert the one hot encoded dataframe to a matrix\n",
    "#matrix = df_one_hot_encoded.to_numpy()\n",
    "\n",
    "# Display the matrix\n",
    "#print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403853b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['Disaster Subgroup','Country', 'Disaster Type', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n",
    "#df3 = pd.get_dummies(df, columns=['Disaster Subgroup', 'Disaster Type', 'ISO', 'Region', 'Continent','Dis Mag Scale','Timezone'])\n",
    "#Creating a Label Encoder\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# le = LabelEncoder()\n",
    "# df['Country']= le.fit_transform(df['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "070a4f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of        Unnamed: 0  Aid Contribution  Dis Mag Value   Latitude  Longitude  \\\n",
       "0               0      5.293972e+07              1  16.000055 -24.008395   \n",
       "1               1      5.339187e+07              1  22.351115  78.667743   \n",
       "2               2      6.032294e+07              8  15.585555 -90.345759   \n",
       "3               7      5.515770e+03            205  24.476929  90.293441   \n",
       "4               9      6.093499e+07              8  22.351115  78.667743   \n",
       "...           ...               ...            ...        ...        ...   \n",
       "14931       16121      4.454193e+07              1  16.347124  47.891527   \n",
       "14932       16122      5.467111e+07              1 -28.816624  24.991639   \n",
       "14933       16123      7.229931e+07           2900  -2.981434  23.822264   \n",
       "14934       16124      5.139031e+06              1  44.153412  20.551440   \n",
       "14935       16125      4.255139e+07              1   7.869943  29.666790   \n",
       "\n",
       "       Total Deaths    No Injured   No Affected   No Homeless  Total Affected  \\\n",
       "0      2.378395e+06  1.157057e+06  2.121271e+08  1.018853e+07    2.121271e+08   \n",
       "1      2.398708e+06  1.166939e+06  2.139388e+08  1.027555e+07    2.139388e+08   \n",
       "2      2.710097e+06  1.318426e+06  2.417113e+08  1.160947e+07    2.417113e+08   \n",
       "3      2.487236e+02  1.215141e+02  2.209527e+04  1.067194e+03    2.209527e+04   \n",
       "4      2.737594e+06  1.331803e+06  2.441638e+08  1.172726e+07    2.441638e+08   \n",
       "...             ...           ...           ...           ...             ...   \n",
       "14931  2.001112e+06  9.735141e+05  1.784775e+08  8.572331e+06    1.784775e+08   \n",
       "14932  2.456180e+06  1.194899e+06  2.190647e+08  1.052174e+07    2.190647e+08   \n",
       "14933  3.248152e+06  1.580182e+06  2.897001e+08  1.391438e+07    2.897001e+08   \n",
       "14934  2.308794e+05  1.123202e+05  2.059186e+07  9.890388e+05    2.059186e+07   \n",
       "14935  1.911684e+06  9.300085e+05  1.705015e+08  8.189240e+06    1.705015e+08   \n",
       "\n",
       "       ...  scaled_Dis Mag Value  scaled_Latitude  scaled_Longitude  \\\n",
       "0      ...              0.000004         0.539987          0.428283   \n",
       "1      ...              0.000004         0.599630          0.717217   \n",
       "2      ...              0.000005         0.536095          0.241608   \n",
       "3      ...              0.000020         0.619594          0.749932   \n",
       "4      ...              0.000005         0.599630          0.717217   \n",
       "...    ...                   ...              ...               ...   \n",
       "14931  ...              0.000004         0.543247          0.630612   \n",
       "14932  ...              0.000004         0.119111          0.566171   \n",
       "14933  ...              0.000227         0.361731          0.562880   \n",
       "14934  ...              0.000004         0.804377          0.553676   \n",
       "14935  ...              0.000004         0.463637          0.579327   \n",
       "\n",
       "      scaled_Total Deaths scaled_No Injured  scaled_No Affected  \\\n",
       "0                0.642785          0.642785            0.642785   \n",
       "1                0.648276          0.648276            0.648276   \n",
       "2                0.732441          0.732441            0.732441   \n",
       "3                0.000000          0.000000            0.000000   \n",
       "4                0.739873          0.739873            0.739873   \n",
       "...                   ...               ...                 ...   \n",
       "14931            0.540810          0.540810            0.540810   \n",
       "14932            0.663810          0.663810            0.663810   \n",
       "14933            0.877871          0.877871            0.877871   \n",
       "14934            0.062337          0.062337            0.062337   \n",
       "14935            0.516639          0.516639            0.516639   \n",
       "\n",
       "       scaled_No Homeless  scaled_Total Affected  scaled_CPI  scaled_Rank  \n",
       "0                0.642785               0.642785    0.000000     0.642785  \n",
       "1                0.648276               0.648276    0.000000     0.648276  \n",
       "2                0.732441               0.732441    0.001332     0.732441  \n",
       "3                0.000000               0.000000    0.002663     0.000000  \n",
       "4                0.739873               0.739873    0.002663     0.739873  \n",
       "...                   ...                    ...         ...          ...  \n",
       "14931            0.540810               0.540810    1.000000     0.540810  \n",
       "14932            0.663810               0.663810    1.000000     0.663810  \n",
       "14933            0.877871               0.877871    1.000000     0.877871  \n",
       "14934            0.062337               0.062337    1.000000     0.062337  \n",
       "14935            0.516639               0.516639    1.000000     0.516639  \n",
       "\n",
       "[14936 rows x 727 columns]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Assuming your data is stored in a Pandas dataframe called \"df\"\n",
    "\n",
    "# Select the features that you want to scale\n",
    "features = df[['Dis Mag Value', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'CPI', 'Rank']]\n",
    "\n",
    "# Iterate through each feature\n",
    "for feature in features:\n",
    "  # Select the feature\n",
    "  col = df[feature]\n",
    "\n",
    "  # Find the minimum and maximum values for the column\n",
    "  min_value = col.min()\n",
    "  max_value = col.max()\n",
    "\n",
    "  # Subtract the minimum value from the column\n",
    "  scaled_col = col - min_value\n",
    "\n",
    "  # Divide by the range (max - min)\n",
    "  scaled_col = scaled_col / (max_value - min_value)\n",
    "\n",
    "  # Assign the scaled values to a new column in the dataframe\n",
    "  df[f'scaled_{feature}'] = scaled_col\n",
    "\n",
    "\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecf747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970de087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725f1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b98d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Aid Contribution</th>\n",
       "      <th>Dis Mag Value</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Total Deaths</th>\n",
       "      <th>No Injured</th>\n",
       "      <th>No Affected</th>\n",
       "      <th>No Homeless</th>\n",
       "      <th>Total Affected</th>\n",
       "      <th>...</th>\n",
       "      <th>scaled_Dis Mag Value</th>\n",
       "      <th>scaled_Latitude</th>\n",
       "      <th>scaled_Longitude</th>\n",
       "      <th>scaled_Total Deaths</th>\n",
       "      <th>scaled_No Injured</th>\n",
       "      <th>scaled_No Affected</th>\n",
       "      <th>scaled_No Homeless</th>\n",
       "      <th>scaled_Total Affected</th>\n",
       "      <th>scaled_CPI</th>\n",
       "      <th>scaled_Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.293972e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>16.000055</td>\n",
       "      <td>-24.008395</td>\n",
       "      <td>2.378395e+06</td>\n",
       "      <td>1.157057e+06</td>\n",
       "      <td>2.121271e+08</td>\n",
       "      <td>1.018853e+07</td>\n",
       "      <td>2.121271e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.539987</td>\n",
       "      <td>0.428283</td>\n",
       "      <td>0.642785</td>\n",
       "      <td>0.642785</td>\n",
       "      <td>0.642785</td>\n",
       "      <td>0.642785</td>\n",
       "      <td>0.642785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5.339187e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>22.351115</td>\n",
       "      <td>78.667743</td>\n",
       "      <td>2.398708e+06</td>\n",
       "      <td>1.166939e+06</td>\n",
       "      <td>2.139388e+08</td>\n",
       "      <td>1.027555e+07</td>\n",
       "      <td>2.139388e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.599630</td>\n",
       "      <td>0.717217</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.648276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.648276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.032294e+07</td>\n",
       "      <td>8</td>\n",
       "      <td>15.585555</td>\n",
       "      <td>-90.345759</td>\n",
       "      <td>2.710097e+06</td>\n",
       "      <td>1.318426e+06</td>\n",
       "      <td>2.417113e+08</td>\n",
       "      <td>1.160947e+07</td>\n",
       "      <td>2.417113e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.536095</td>\n",
       "      <td>0.241608</td>\n",
       "      <td>0.732441</td>\n",
       "      <td>0.732441</td>\n",
       "      <td>0.732441</td>\n",
       "      <td>0.732441</td>\n",
       "      <td>0.732441</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.732441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>5.515770e+03</td>\n",
       "      <td>205</td>\n",
       "      <td>24.476929</td>\n",
       "      <td>90.293441</td>\n",
       "      <td>2.487236e+02</td>\n",
       "      <td>1.215141e+02</td>\n",
       "      <td>2.209527e+04</td>\n",
       "      <td>1.067194e+03</td>\n",
       "      <td>2.209527e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.619594</td>\n",
       "      <td>0.749932</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>6.093499e+07</td>\n",
       "      <td>8</td>\n",
       "      <td>22.351115</td>\n",
       "      <td>78.667743</td>\n",
       "      <td>2.737594e+06</td>\n",
       "      <td>1.331803e+06</td>\n",
       "      <td>2.441638e+08</td>\n",
       "      <td>1.172726e+07</td>\n",
       "      <td>2.441638e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.599630</td>\n",
       "      <td>0.717217</td>\n",
       "      <td>0.739873</td>\n",
       "      <td>0.739873</td>\n",
       "      <td>0.739873</td>\n",
       "      <td>0.739873</td>\n",
       "      <td>0.739873</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.739873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14931</th>\n",
       "      <td>16121</td>\n",
       "      <td>4.454193e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>16.347124</td>\n",
       "      <td>47.891527</td>\n",
       "      <td>2.001112e+06</td>\n",
       "      <td>9.735141e+05</td>\n",
       "      <td>1.784775e+08</td>\n",
       "      <td>8.572331e+06</td>\n",
       "      <td>1.784775e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.543247</td>\n",
       "      <td>0.630612</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>0.540810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.540810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14932</th>\n",
       "      <td>16122</td>\n",
       "      <td>5.467111e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>-28.816624</td>\n",
       "      <td>24.991639</td>\n",
       "      <td>2.456180e+06</td>\n",
       "      <td>1.194899e+06</td>\n",
       "      <td>2.190647e+08</td>\n",
       "      <td>1.052174e+07</td>\n",
       "      <td>2.190647e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.119111</td>\n",
       "      <td>0.566171</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>0.663810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.663810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14933</th>\n",
       "      <td>16123</td>\n",
       "      <td>7.229931e+07</td>\n",
       "      <td>2900</td>\n",
       "      <td>-2.981434</td>\n",
       "      <td>23.822264</td>\n",
       "      <td>3.248152e+06</td>\n",
       "      <td>1.580182e+06</td>\n",
       "      <td>2.897001e+08</td>\n",
       "      <td>1.391438e+07</td>\n",
       "      <td>2.897001e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.361731</td>\n",
       "      <td>0.562880</td>\n",
       "      <td>0.877871</td>\n",
       "      <td>0.877871</td>\n",
       "      <td>0.877871</td>\n",
       "      <td>0.877871</td>\n",
       "      <td>0.877871</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14934</th>\n",
       "      <td>16124</td>\n",
       "      <td>5.139031e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>44.153412</td>\n",
       "      <td>20.551440</td>\n",
       "      <td>2.308794e+05</td>\n",
       "      <td>1.123202e+05</td>\n",
       "      <td>2.059186e+07</td>\n",
       "      <td>9.890388e+05</td>\n",
       "      <td>2.059186e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.804377</td>\n",
       "      <td>0.553676</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>0.062337</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14935</th>\n",
       "      <td>16125</td>\n",
       "      <td>4.255139e+07</td>\n",
       "      <td>1</td>\n",
       "      <td>7.869943</td>\n",
       "      <td>29.666790</td>\n",
       "      <td>1.911684e+06</td>\n",
       "      <td>9.300085e+05</td>\n",
       "      <td>1.705015e+08</td>\n",
       "      <td>8.189240e+06</td>\n",
       "      <td>1.705015e+08</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.463637</td>\n",
       "      <td>0.579327</td>\n",
       "      <td>0.516639</td>\n",
       "      <td>0.516639</td>\n",
       "      <td>0.516639</td>\n",
       "      <td>0.516639</td>\n",
       "      <td>0.516639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.516639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14936 rows Ã— 727 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Aid Contribution  Dis Mag Value   Latitude  Longitude  \\\n",
       "0               0      5.293972e+07              1  16.000055 -24.008395   \n",
       "1               1      5.339187e+07              1  22.351115  78.667743   \n",
       "2               2      6.032294e+07              8  15.585555 -90.345759   \n",
       "3               7      5.515770e+03            205  24.476929  90.293441   \n",
       "4               9      6.093499e+07              8  22.351115  78.667743   \n",
       "...           ...               ...            ...        ...        ...   \n",
       "14931       16121      4.454193e+07              1  16.347124  47.891527   \n",
       "14932       16122      5.467111e+07              1 -28.816624  24.991639   \n",
       "14933       16123      7.229931e+07           2900  -2.981434  23.822264   \n",
       "14934       16124      5.139031e+06              1  44.153412  20.551440   \n",
       "14935       16125      4.255139e+07              1   7.869943  29.666790   \n",
       "\n",
       "       Total Deaths    No Injured   No Affected   No Homeless  Total Affected  \\\n",
       "0      2.378395e+06  1.157057e+06  2.121271e+08  1.018853e+07    2.121271e+08   \n",
       "1      2.398708e+06  1.166939e+06  2.139388e+08  1.027555e+07    2.139388e+08   \n",
       "2      2.710097e+06  1.318426e+06  2.417113e+08  1.160947e+07    2.417113e+08   \n",
       "3      2.487236e+02  1.215141e+02  2.209527e+04  1.067194e+03    2.209527e+04   \n",
       "4      2.737594e+06  1.331803e+06  2.441638e+08  1.172726e+07    2.441638e+08   \n",
       "...             ...           ...           ...           ...             ...   \n",
       "14931  2.001112e+06  9.735141e+05  1.784775e+08  8.572331e+06    1.784775e+08   \n",
       "14932  2.456180e+06  1.194899e+06  2.190647e+08  1.052174e+07    2.190647e+08   \n",
       "14933  3.248152e+06  1.580182e+06  2.897001e+08  1.391438e+07    2.897001e+08   \n",
       "14934  2.308794e+05  1.123202e+05  2.059186e+07  9.890388e+05    2.059186e+07   \n",
       "14935  1.911684e+06  9.300085e+05  1.705015e+08  8.189240e+06    1.705015e+08   \n",
       "\n",
       "       ...  scaled_Dis Mag Value  scaled_Latitude  scaled_Longitude  \\\n",
       "0      ...              0.000004         0.539987          0.428283   \n",
       "1      ...              0.000004         0.599630          0.717217   \n",
       "2      ...              0.000005         0.536095          0.241608   \n",
       "3      ...              0.000020         0.619594          0.749932   \n",
       "4      ...              0.000005         0.599630          0.717217   \n",
       "...    ...                   ...              ...               ...   \n",
       "14931  ...              0.000004         0.543247          0.630612   \n",
       "14932  ...              0.000004         0.119111          0.566171   \n",
       "14933  ...              0.000227         0.361731          0.562880   \n",
       "14934  ...              0.000004         0.804377          0.553676   \n",
       "14935  ...              0.000004         0.463637          0.579327   \n",
       "\n",
       "      scaled_Total Deaths scaled_No Injured  scaled_No Affected  \\\n",
       "0                0.642785          0.642785            0.642785   \n",
       "1                0.648276          0.648276            0.648276   \n",
       "2                0.732441          0.732441            0.732441   \n",
       "3                0.000000          0.000000            0.000000   \n",
       "4                0.739873          0.739873            0.739873   \n",
       "...                   ...               ...                 ...   \n",
       "14931            0.540810          0.540810            0.540810   \n",
       "14932            0.663810          0.663810            0.663810   \n",
       "14933            0.877871          0.877871            0.877871   \n",
       "14934            0.062337          0.062337            0.062337   \n",
       "14935            0.516639          0.516639            0.516639   \n",
       "\n",
       "       scaled_No Homeless  scaled_Total Affected  scaled_CPI  scaled_Rank  \n",
       "0                0.642785               0.642785    0.000000     0.642785  \n",
       "1                0.648276               0.648276    0.000000     0.648276  \n",
       "2                0.732441               0.732441    0.001332     0.732441  \n",
       "3                0.000000               0.000000    0.002663     0.000000  \n",
       "4                0.739873               0.739873    0.002663     0.739873  \n",
       "...                   ...                    ...         ...          ...  \n",
       "14931            0.540810               0.540810    1.000000     0.540810  \n",
       "14932            0.663810               0.663810    1.000000     0.663810  \n",
       "14933            0.877871               0.877871    1.000000     0.877871  \n",
       "14934            0.062337               0.062337    1.000000     0.062337  \n",
       "14935            0.516639               0.516639    1.000000     0.516639  \n",
       "\n",
       "[14936 rows x 727 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8bdfaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Disaster Subgroup', 'Disaster Type', 'Country', 'ISO',\n",
       "       'Region', 'Continent', 'Aid Contribution', 'Dis Mag Value',\n",
       "       'Dis Mag Scale', 'Latitude', 'Longitude', 'Timezone', 'Total Deaths',\n",
       "       'No Injured', 'No Affected', 'No Homeless', 'Total Affected',\n",
       "       'Insured Damages ('000 US$)', 'Total Damages ('000 US$)', 'CPI',\n",
       "       'Start Date', 'End Date', 'Rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90f3a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'Aid Contribution', 'Dis Mag Value', 'Latitude',\n",
      "       'Longitude', 'Total Deaths', 'No Injured', 'No Affected', 'No Homeless',\n",
      "       'Total Affected',\n",
      "       ...\n",
      "       'scaled_Dis Mag Value', 'scaled_Latitude', 'scaled_Longitude',\n",
      "       'scaled_Total Deaths', 'scaled_No Injured', 'scaled_No Affected',\n",
      "       'scaled_No Homeless', 'scaled_Total Affected', 'scaled_CPI',\n",
      "       'scaled_Rank'],\n",
      "      dtype='object', length=727)\n"
     ]
    }
   ],
   "source": [
    "#X = df3.drop(['Total Deaths','Start Date','End Date','Rank'], axis=1)  \n",
    "#y = df3['Total Deaths']\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45350402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming your data is stored in a Pandas dataframe called \"df\"\n",
    "\n",
    "# Select the features that you want to scale\n",
    "#features = df3[['Dis Mag Value', 'Latitude', 'Longitude', 'Total Deaths',\n",
    " #      'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'CPI', 'Rank']]\n",
    "\n",
    "# Iterate through each feature\n",
    "#for feature in features:\n",
    "  # Select the feature\n",
    "#  col = df3[feature]\n",
    "\n",
    "  # Find the minimum and maximum values for the column\n",
    " # min_value = col.min()\n",
    " # max_value = col.max()\n",
    "\n",
    "  # Subtract the minimum value from the column\n",
    " # scaled_col = col - min_value\n",
    "\n",
    "  # Divide by the range (max - min)\n",
    " # scaled_col = scaled_col / (max_value - min_value)\n",
    "\n",
    "  # Assign the scaled values to a new column in the dataframe\n",
    " # df3[f'scaled_{feature}'] = scaled_col\n",
    "#df3.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d504f2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unnamed: 0' 'Disaster Subgroup' 'Disaster Type' 'Country' 'ISO' 'Region'\n",
      " 'Continent' 'Aid Contribution' 'Dis Mag Value' 'Dis Mag Scale' 'Latitude'\n",
      " 'Longitude' 'Timezone' 'Total Deaths' 'No Injured' 'No Affected'\n",
      " 'No Homeless' 'Total Affected' \"Insured Damages ('000 US$)\"\n",
      " \"Total Damages ('000 US$)\" 'CPI' 'Start Date' 'End Date' 'Rank']\n"
     ]
    }
   ],
   "source": [
    "df.columns\n",
    "print(df2.columns.values)\n",
    "#df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4a8edd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# # Load the data and split it into training and testing sets\n",
    "# X = df2[['Aid Contribution', 'Dis Mag Value', 'Latitude', 'Longitude']]\n",
    "# y = df2[['Year', 'Country']]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Train a KNN classifier\n",
    "# knn = KNeighborsClassifier()\n",
    "# knn.fit(X_train, y_train)\n",
    "\n",
    "# # Evaluate the model on the testing set\n",
    "# accuracy = knn.score(X_test, y_test)\n",
    "# print(\"Accuracy: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e314e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e8660ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the 'Date' column to a datetime type\n",
    "# df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# # Extract the month from each date\n",
    "# df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "# #F\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the dataset and extract the input features and target variable\n",
    "# #X = dfle[['ISO','Total Deaths','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "# #y = dfle['Country']\n",
    "\n",
    "# X = df3.drop(['Total Deaths','Country','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "# #y = df3['Rank']\n",
    "# y = F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Split the data into a training set and a test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# # Train a random forest classifier on the training set\n",
    "# model = RandomForestClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Evaluate the model's performance using accuracy\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38f12b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 15440211.0000 - mae: 2541.2366\n",
      "Epoch 2/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 759937.1875 - mae: 534.0903\n",
      "Epoch 3/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 192002.7500 - mae: 176.6137\n",
      "Epoch 4/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 100593.6562 - mae: 116.2761\n",
      "Epoch 5/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 65353.6914 - mae: 90.2724\n",
      "Epoch 6/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 47102.7695 - mae: 74.4208\n",
      "Epoch 7/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 37303.9805 - mae: 64.3674\n",
      "Epoch 8/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 30470.4902 - mae: 58.4748\n",
      "Epoch 9/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 25915.8027 - mae: 54.4795\n",
      "Epoch 10/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 21737.0254 - mae: 49.2106\n",
      "Epoch 11/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 21600.7598 - mae: 47.7362\n",
      "Epoch 12/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 20693.8633 - mae: 48.9141\n",
      "Epoch 13/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 17348.2715 - mae: 45.6821\n",
      "Epoch 14/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 18135.3379 - mae: 45.5900\n",
      "Epoch 15/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 17811.0801 - mae: 43.2762\n",
      "Epoch 16/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 15643.5488 - mae: 43.6005\n",
      "Epoch 17/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 15386.7412 - mae: 41.7174\n",
      "Epoch 18/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 15275.6709 - mae: 43.7269\n",
      "Epoch 19/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 14206.9541 - mae: 42.0879\n",
      "Epoch 20/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 13449.5137 - mae: 39.4712\n",
      "Epoch 21/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 12392.8623 - mae: 39.2864\n",
      "Epoch 22/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 12061.9365 - mae: 39.4583\n",
      "Epoch 23/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 11741.0576 - mae: 37.7136\n",
      "Epoch 24/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 10979.2393 - mae: 35.5389\n",
      "Epoch 25/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 11145.1133 - mae: 38.0694\n",
      "Epoch 26/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 11135.5664 - mae: 38.7268\n",
      "Epoch 27/50\n",
      "374/374 [==============================] - 0s 1ms/step - loss: 10348.4512 - mae: 35.0256\n",
      "Epoch 28/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 9596.5869 - mae: 34.2324\n",
      "Epoch 29/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 9210.1221 - mae: 33.2855\n",
      "Epoch 30/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 9027.9287 - mae: 31.7528\n",
      "Epoch 31/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 9748.2715 - mae: 33.9088\n",
      "Epoch 32/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 8598.4453 - mae: 33.0068\n",
      "Epoch 33/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 8033.4341 - mae: 31.9044\n",
      "Epoch 34/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 8138.0415 - mae: 31.1540\n",
      "Epoch 35/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 8670.4434 - mae: 33.1628\n",
      "Epoch 36/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 7717.2168 - mae: 30.9936\n",
      "Epoch 37/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 8168.3569 - mae: 30.7263\n",
      "Epoch 38/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 7689.9512 - mae: 32.0516\n",
      "Epoch 39/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 6566.3179 - mae: 28.4654\n",
      "Epoch 40/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 7468.7231 - mae: 31.0064\n",
      "Epoch 41/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 7290.0723 - mae: 33.4253\n",
      "Epoch 42/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 6616.0420 - mae: 30.8851\n",
      "Epoch 43/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 6378.1924 - mae: 30.2951\n",
      "Epoch 44/50\n",
      "374/374 [==============================] - 1s 2ms/step - loss: 6678.3872 - mae: 32.5656\n",
      "Epoch 45/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 6318.7393 - mae: 29.1646\n",
      "Epoch 46/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 5766.8608 - mae: 26.9536\n",
      "Epoch 47/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 6326.2646 - mae: 28.1824\n",
      "Epoch 48/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 6766.6099 - mae: 32.3095\n",
      "Epoch 49/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 5226.0483 - mae: 25.6426\n",
      "Epoch 50/50\n",
      "374/374 [==============================] - 1s 1ms/step - loss: 5591.6128 - mae: 29.7334\n",
      "94/94 [==============================] - 0s 943us/step - loss: 15409.7754 - mae: 35.1037\n",
      "loss\n",
      "mae\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# select the features and target columns\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value'\n",
    "             ,'Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scale the input data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "model.add(Dense(2, activation='linear'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, return_dict=True)\n",
    "#print(f'Test loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}')\n",
    "#print('Test loss: {:.4f}, Test MAE: {:.4f}'.format(test_loss, test_mae))\n",
    "print(test_loss)\n",
    "print(test_mae)\n",
    "\n",
    "\n",
    "# use the model to make predictions on new data\n",
    "#new_data = np.array([[...]]) # replace with actual data\n",
    "#predictions = model.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49094eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Need to preprocess the data first\n",
    "\n",
    "# # create a Pandas data frame with the input data for the new samples\n",
    "# lol = pd.read_csv('2022_DISASTERS_TARGET.csv')\n",
    "\n",
    "\n",
    "# # convert the data frame to a NumPy array\n",
    "# new_data = lol.values\n",
    "\n",
    "# # use the model to make predictions on the new data\n",
    "# predictions = model.predict(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e32d8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# extract the MAE values from the history\n",
    "mae = history.history['mae']\n",
    "\n",
    "# plot the MAE values\n",
    "#plt.plot(mae)\n",
    "#plt.xlabel('Epoch')\n",
    "#plt.ylabel('MAE')\n",
    "#plt.show()\n",
    "\n",
    "# create a scatter plot with the epoch numbers on the x-axis and the MAE values on the y-axis\n",
    "plt.scatter(range(1, len(mae) + 1), mae)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()\n",
    "##ITs gonna take some time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abd720b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7312f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Need to preprocess and work on the data below \n",
    "### Code above this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77dcd567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Need to preprocess the data first\n",
    "\n",
    "# # create a Pandas data frame with the input data for the new samples\n",
    "# lol = pd.read_csv('2022_DISASTERS_TARGET.csv')\n",
    "\n",
    "\n",
    "# # convert the data frame to a NumPy array\n",
    "# new_data = lol.values\n",
    "\n",
    "# # use the model to make predictions on the new data\n",
    "# predictions = model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1956ffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.0471140423169567e-16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "#X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Total Deaths']\n",
    "\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "#y = df[ 'scaled_Dis Mag Value']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a065a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "299/299 [==============================] - 1s 2ms/step - loss: 37184432.0000 - accuracy: 0.0017 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184924.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184912.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184920.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184924.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184920.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184924.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37184932.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37184924.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37184904.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37184936.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/40\n",
      "299/299 [==============================] - 1s 2ms/step - loss: 37184936.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37184936.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184936.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184936.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/40\n",
      "299/299 [==============================] - 0s 2ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184920.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184920.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184936.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184920.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184932.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184924.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184928.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/40\n",
      "299/299 [==============================] - 0s 1ms/step - loss: 37184908.0000 - accuracy: 2.6156e-04 - val_loss: 37505416.0000 - val_accuracy: 0.0000e+00\n",
      "94/94 [==============================] - 0s 903us/step - loss: 36853380.0000 - accuracy: 0.0000e+00\n",
      "Test loss: 36853380.0000\n",
      "Test accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "## Trying to predict the accuracy but a regression model doesnt predict accuracy \n",
    "\n",
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = df.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = df[['Rank', 'scaled_Dis Mag Value']]\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_logarithmic_error'])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f9eceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # Select the categorical columns\n",
    "# cat_cols = df2[['Disaster Subgroup', 'Disaster Type', 'Country', 'ISO', 'Region', 'Continent','Dis Mag Scale']]\n",
    "\n",
    "# # Create the OneHotEncoder object\n",
    "# onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# # One-hot encode the categorical columns\n",
    "# onehot_encoded = onehot_encoder.fit_transform(df2[cat_cols])\n",
    "\n",
    "# # Add the one-hot encoded columns to the original dataframe\n",
    "# df2_onehot = pd.concat([df2, pd.DataFrame(onehot_encoded)], axis=1)\n",
    "\n",
    "# # Drop the categorical columns\n",
    "# df2_onehot = df2_onehot.drop(cat_cols, axis=1)\n",
    "\n",
    "\n",
    "# # Display the resulting dataframe\n",
    "# df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bddf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Select the numerical columns\n",
    "# num_cols = ['Aid Contribution', 'Dis Mag Value', 'Dis Mag Scale', 'Latitude', 'Longitude', 'Total Deaths',\n",
    "#             'No Injured', 'No Affected', 'No Homeless', 'Total Affected', 'Insured Damages ('000 US$)', \n",
    "#             'Total Damages ('000 US$)', 'CPI']\n",
    "\n",
    "# # Create the MinMaxScaler object\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Scale the numerical columns\n",
    "# df[num_cols] = scaler.fit_transform(df[num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81563cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1070e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not reliable for machine learning \n",
    "#Creating a Label Encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbcb562a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Disaster Subgroup', 'Disaster Type', 'Disaster Subtype',\n",
       "       'Disaster Subsubtype', 'Event Name', 'Country', 'ISO', 'Region',\n",
       "       'Continent', 'Location', 'Origin', 'Associated Dis', 'Associated Dis2',\n",
       "       'OFDA Response', 'Appeal', 'Declaration', 'Aid Contribution',\n",
       "       'Dis Mag Value', 'Dis Mag Scale', 'Latitude', 'Longitude', 'Timezone',\n",
       "       'River Basin', 'Total Deaths', 'No Injured', 'No Affected',\n",
       "       'No Homeless', 'Total Affected', 'Insured Damages ('000 US$)',\n",
       "       'Total Damages ('000 US$)', 'CPI', 'Start Date', 'End Date', 'Rank'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Encdoign the Stuff using label \n",
    "#Changing categorical values to numerical values\n",
    "dfle = pd.read_csv('1900_2021_DISASTERS_INTERPOLATED.csv')\n",
    "dfle.Continent = le.fit_transform(dfle.Continent)\n",
    "dfle.ISO= le.fit_transform(dfle.ISO)\n",
    "dfle.Region= le.fit_transform(dfle.Region)\n",
    "dfle['Disaster Type']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "dfle['Disaster Subgroup']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "dfle['Start Date']= le.fit_transform(dfle['Start Date'])\n",
    "dfle['End Date']= le.fit_transform(dfle['End Date'])\n",
    "dfle['Timezone']= le.fit_transform(dfle['Timezone'])\n",
    "dfle['Country']= le.fit_transform(dfle['Country'])\n",
    "dfle['Dis Mag Scale']= le.fit_transform(dfle['Dis Mag Scale'])\n",
    "\n",
    "dfle.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d5c5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(df3.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3729bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the 'Date' column to a datetime type\n",
    "# df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# # Extract the month from each date\n",
    "# df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "# F = df3[['Year','Longitude','Latitude']]\n",
    "# #F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555682b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "##### Using label encoding\n",
    "#X = merged.drop(columns =['Disaster Subgroup', 'Disaster Type','Country','ISO','Region','Continent'])\n",
    "#y = dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Timezone','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "#X = df3.drop(['Total Deaths'], axis=1)  \n",
    "#y = df3['Total Deaths']\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7366092",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'drought'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train a random forest classifier on the training set\u001b[39;00m\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:327\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 327\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2069\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'drought'"
     ]
    }
   ],
   "source": [
    "# # Convert the 'Date' column to a datetime type\n",
    "# df3['Start Date'] = pd.to_datetime(df3['Start Date'])\n",
    "\n",
    "# # Extract the month from each date\n",
    "# df3['Year'] = df3['Start Date'].dt.year\n",
    "\n",
    "# F = df3[['Year','Longitude','Latitude']]\n",
    "# #F\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset and extract the input features and target variable\n",
    "#X = dfle[['ISO','Total Deaths','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "#y = dfle['Country']\n",
    "\n",
    "X = dfle.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = dfle['Rank']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train a random forest classifier on the training set\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance using accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52ff753",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5014bc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umerf\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py:1696: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  return t[start:end]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "299/299 [==============================] - 4s 1ms/step - loss: -32274016349716480.0000 - accuracy: 0.0000e+00 - val_loss: -192479921442914304.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: -1561420860651208704.0000 - accuracy: 0.0000e+00 - val_loss: -4336401567059017728.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      "299/299 [==============================] - 0s 910us/step - loss: -11711835131349041152.0000 - accuracy: 0.0000e+00 - val_loss: -22879924379367505920.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      "299/299 [==============================] - 0s 987us/step - loss: -42817159818620436480.0000 - accuracy: 0.0000e+00 - val_loss: -69700799659883102208.0000 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      "299/299 [==============================] - 0s 980us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      "299/299 [==============================] - 0s 938us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      "299/299 [==============================] - 0s 957us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      "299/299 [==============================] - 0s 902us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      "299/299 [==============================] - 0s 983us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      "299/299 [==============================] - 0s 983us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      "299/299 [==============================] - 0s 980us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      "299/299 [==============================] - 0s 939us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      "299/299 [==============================] - 0s 943us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      "299/299 [==============================] - 0s 876us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      "299/299 [==============================] - 0s 963us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      "299/299 [==============================] - 0s 950us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      "299/299 [==============================] - 0s 997us/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      "299/299 [==============================] - 0s 1ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00\n",
      "94/94 [==============================] - 0s 667us/step - loss: nan - accuracy: 0.0000e+00\n",
      "Test loss: nan\n",
      "Test accuracy: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "#import tensorflow as tf\n",
    "#import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the data\n",
    "#X =dfle[['ISO', 'Region','Continent','Disaster Type']]\n",
    "#y =  dfle['Disaster Subgroup']\n",
    "#X = dfle[['ISO', 'Disaster Subgroup','Continent','Country','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "#y = dfle['Timezone']\n",
    "\n",
    "X = dfle[['ISO','Country','Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Timezone','Dis Mag Value','Dis Mag Scale','Aid Contribution','No Injured','No Affected','No Homeless','Total Affected','CPI']]\n",
    "y = dfle['Total Deaths']\n",
    "\n",
    "\n",
    "# Encode the string feature as a numerical value\n",
    "# Trying to convert string to a numerical Value for the nueral network to process\n",
    "#encoder = LabelEncoder()\n",
    "#X = encoder.fit_transform(X)\n",
    "#y = encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test loss: {loss:.4f}\")\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b049122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umerf\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume you have a dataframe 'df' with columns 'feature1', 'feature2', ..., 'target'\n",
    "# and a list of the column names of the features called 'feature_cols'\n",
    "\n",
    "# Split the data into features and target\n",
    "X = dfle[['ISO', 'Disaster Subgroup','Continent','Disaster Type','Latitude','Longitude','Region','Start Date','End Date']]\n",
    "y = dfle['Timezone']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the 'predict' method to make predictions on the test data\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# You can also use the 'predict_proba' method to predict the class probabilities\n",
    "probabilities = logreg.predict_proba(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test data using the 'accuracy_score' function from the 'sklearn.metrics' module\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c385ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Changing categorical values to numerical values\n",
    "# dfle = df\n",
    "# dfle.Continent = le.fit_transform(dfle.Continent)\n",
    "# dfle.ISO= le.fit_transform(dfle.ISO)\n",
    "# dfle.Region= le.fit_transform(dfle.Region)\n",
    "# dfle['Disaster Type']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "# dfle['Disaster Subgroup']= le.fit_transform(dfle['Disaster Subgroup'])\n",
    "# dfle['Start Date']= le.fit_transform(dfle['Start Date'])\n",
    "# dfle['End Date']= le.fit_transform(dfle['End Date'])\n",
    "# dfle['Timezone']= le.fit_transform(dfle['Timezone'])\n",
    "# dfle['Country']= le.fit_transform(dfle['Country'])\n",
    "# dfle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc332bff",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'tropical cyclone'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 13\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train a KNN classifier\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:852\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    851\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:806\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:841\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \n\u001b[0;32m    811\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    839\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    840\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 841\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2069\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'tropical cyclone'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the data and split it into training and testing sets\n",
    "X = dfle.drop(['Total Deaths','Start Date','End Date','Dis Mag Value','Total Deaths','No Injured','No Homeless','No Affected','Total Affected','Rank'], axis=1)  \n",
    "#y = df3['Rank']\n",
    "y = dfle['Rank']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99aa072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
